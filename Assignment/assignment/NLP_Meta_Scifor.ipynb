{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NLP Introduction"
      ],
      "metadata": {
        "id": "D7xvweEQay01"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZwhxU6TV4t0",
        "outputId": "60f4be69-01a1-4947-b968-5a386af88ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ArYAn\n"
          ]
        }
      ],
      "source": [
        "a = \"ArYAn\"\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"DATASET\"\n",
        "print(a[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqQL3_XtW0sM",
        "outputId": "71b5f53e-3d17-4988-95f2-cb1bffa6eeb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"Data test\"\n",
        "print(a[3:6])\n",
        "print(\"-\" * 50)\n",
        "print(a[-6:-2])\n",
        "print(\"-\" * 50)\n",
        "print(a[2:6:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8FLkYluXTQp",
        "outputId": "f9c3d035-e948-4083-dd6a-6c8b798a2613"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a t\n",
            "--------------------------------------------------\n",
            "a te\n",
            "--------------------------------------------------\n",
            "t \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMAnYB_mXs74",
        "outputId": "b54c2fad-25fe-420d-9f51-2578e21f37b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Data', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.lower())\n",
        "print(a.upper())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoUe4ppcX0hi",
        "outputId": "f5fae2dd-33d3-4887-dbc0-6b12a365da5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data test\n",
            "DATA TEST\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.replace(\"Data\" , \"me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgeGRTsUX4P6",
        "outputId": "3f1da8c7-5a79-4b8a-c668-0d0a6d009256"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "me test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"data test\"\n",
        "b = \"train\"\n",
        "print(a + \" \" + b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctoh2mSJX8Vr",
        "outputId": "37678751-57f9-435c-c866-54e74777abba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data test train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('dataset.txt' , \"w+\")"
      ],
      "metadata": {
        "id": "dCezHyZ4YJCW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  file.write(\"Line number is %d\\r\\n\" % (i + 1))"
      ],
      "metadata": {
        "id": "zP2nuS8aYUcN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file.close()"
      ],
      "metadata": {
        "id": "Yp1N3pnPYeGD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('dataset.txt' , \"a+\")"
      ],
      "metadata": {
        "id": "yeMvtSrFYgsq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  file.write(\"Line number is %d\\r\\n\" % (i + 1))"
      ],
      "metadata": {
        "id": "IZw7AS7bYmZT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file.close()"
      ],
      "metadata": {
        "id": "yvsOIYEMYqtv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('dataset.txt' , \"r\")"
      ],
      "metadata": {
        "id": "xOwYKWTJYr2i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if file.mode == 'r':\n",
        "  contents = file.read()\n",
        "  print(contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ArxQ_yWYuAO",
        "outputId": "2876d940-6c41-4ec5-8d92-ee7febdec93e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line number is 1\n",
            "Line number is 2\n",
            "Line number is 3\n",
            "Line number is 4\n",
            "Line number is 5\n",
            "Line number is 1\n",
            "Line number is 2\n",
            "Line number is 3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "WsOxXpkfUTjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "62MqBTSWYy_v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text lowercase\n",
        "def lowercase_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "lowercase_text(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1Em25Gy-Uh_i",
        "outputId": "8c0c61ca-ab61-403f-9b16-410ec8d949d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "  result = re.sub(r'\\d+' , '' , text)\n",
        "  return result\n",
        "\n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "remove_numbers(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "smbbnLoQUr7C",
        "outputId": "b5979ffa-5d42-403a-e194-adbcacb0ccae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are  balls in this bag, and  in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inflect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzaOTLUeUz4x",
        "outputId": "f40be65e-baa3-4166-c88f-5c1cf7d76b37"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (7.4.0)\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect) (4.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inflect\n",
        "q = inflect.engine()\n",
        "\n",
        "# convert number into text\n",
        "def convert_number(text):\n",
        "  temp_string = text.split()\n",
        "  new_str = []\n",
        "\n",
        "  for word in temp_string:\n",
        "    if word.isdigit():\n",
        "      temp = q.number_to_words(word)\n",
        "      new_str.append(temp)\n",
        "\n",
        "    else:\n",
        "      new_str.append(word)\n",
        "\n",
        "  temp_str = ' '.join(new_str)\n",
        "  return temp_str\n",
        "\n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "convert_number(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lZaXhpUAU3Se",
        "outputId": "d68bf5a4-9d73-41aa-82ed-4a5d161f31ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are three balls in this bag, and twelve in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuation\n",
        "def remove_punctuation(text):\n",
        "  translator = str.maketrans('' , '' , string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "input_str = \"Hey, Are you excited? :)\"\n",
        "remove_punctuation(input_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "58qIVgVnVQCs",
        "outputId": "1a0182b2-d872-43b6-b3ba-1dbee093b9b1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey Are you excited '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GlWV3n7pVdpt",
        "outputId": "c4bf2248-2c50-4537-968e-3b70f4f0b4d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove default stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXT5WPicVga-",
        "outputId": "e1655efb-7c55-4e3d-c990-b0d5b8bb0d25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M78YxOJ8WAWD",
        "outputId": "3d5500d3-9ed2-472e-b785-3686409cc555"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords function\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "  return filtered_text\n",
        "\n",
        "ex_text = \"This is an example showing off stop word filtration.\"\n",
        "remove_stopwords(ex_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztBwWuPfVs0e",
        "outputId": "a7b936e0-8191-40ad-d148-b4148f58adee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n",
        "Boys ----> Boy\n",
        "going ---> go"
      ],
      "metadata": {
        "id": "teGNZe-8WHYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# stem words in the list of tokenized words\n",
        "def stem_words(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  stems = [stemmer.stem(word) for word in word_tokens]\n",
        "  return stems\n",
        "\n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kvzo8poV6OY",
        "outputId": "fcfe426c-d1b1-478f-990a-44be769bbc5a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'scienc',\n",
              " 'use',\n",
              " 'scientif',\n",
              " 'method',\n",
              " 'algorithm',\n",
              " 'and',\n",
              " 'mani',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "gbDKThSBWa67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZqM-_clW5gc",
        "outputId": "2f52adbc-ac0e-4cd9-fb39-92161bda2fde"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemma = wordnet.WordNetLemmatizer()\n",
        "\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  lemmas = [lemma.lemmatize(word , pos = 'v') for word in word_tokens]\n",
        "  return lemmas\n",
        "\n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "lemmatize_word(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LanMrlT8WY3V",
        "outputId": "c95cec3d-b9de-4c73-8935-d2fb4499089e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'science',\n",
              " 'use',\n",
              " 'scientific',\n",
              " 'methods',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'many',\n",
              " 'type',\n",
              " 'of',\n",
              " 'process']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parts of Speech (POS) tagging"
      ],
      "metadata": {
        "id": "BsLDt0n3W_8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  return pos_tag(word_tokens)\n",
        "\n",
        "pos_tagging('you just gave me a pen')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51F11SQUWuGF",
        "outputId": "28367a07-40be-4780-b554-706641df3cda"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('you', 'PRP'),\n",
              " ('just', 'RB'),\n",
              " ('gave', 'VBD'),\n",
              " ('me', 'PRP'),\n",
              " ('a', 'DT'),\n",
              " ('pen', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset('PRP')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txk2eAcGXKAc",
        "outputId": "216bedb3-504b-4cbd-a130-cef62389a673"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHUNKING\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def chunking(text , grammar):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "  chunkParser = nltk.RegexpParser(grammar)\n",
        "  tree = chunkParser.parse(word_pos)\n",
        "  for subtree in tree.subtrees():\n",
        "    print(subtree)\n",
        "\n",
        "text = 'the little red parrot is flying in the sky'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunking(text , grammar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyqkOzLBXWHt",
        "outputId": "3f23d755-960a-4f7b-849b-719216c95f52"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition"
      ],
      "metadata": {
        "id": "w5hSBENlXpdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def ner(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "  print(ne_chunk(word_pos))\n",
        "\n",
        "text = 'the little red parrot is flying in the sky'\n",
        "ner(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub8Ku38LXdRv",
        "outputId": "83669329-d7b5-49b1-e48a-e65458d21724"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  the/DT\n",
            "  little/JJ\n",
            "  red/JJ\n",
            "  parrot/NN\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  the/DT\n",
            "  sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sent = \"dataset, Data is a new fuel\"\n",
        "r2 = re.findall(r\"^\\w+\", sent)\n",
        "print(r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey4cJH2jX1ul",
        "outputId": "d5ceeafa-4388-4a53-e51a-3497638de657"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "print(re.split(r\"^\\s\", \"dataset, Data is,  a new fuel\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DysdB5LHX7Kz",
        "outputId": "69a39133-3452-4616-8abb-d6bd5d05413f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset, Data is,  a new fuel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "print(re.split(r's', 'We splited this sentence'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcvJ51YjYVKk",
        "outputId": "425e2aa6-1148-47b1-a32e-1a1708aba09c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We ', 'plited thi', ' ', 'entence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using re.match()\n",
        "lists =  ['is', 'a', 'good', 'day']\n",
        "\n",
        "for i in lists:\n",
        "  q = re.match(r\"^\\w+\", i)\n",
        "  if q:\n",
        "    print(q.group())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lERG_W4_Yqby",
        "outputId": "f4a3e083-b003-4be4-8181-e678a8c24f90"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is\n",
            "a\n",
            "good\n",
            "day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "pattern = ['playing' , 'dataset']\n",
        "text = \"Raju is playing outside\"\n",
        "\n",
        "for p in pattern:\n",
        "  print('looking for \"%s\" in \"%s\" ->' % (p , text) , end = \" \")\n",
        "  if re.search(p , text):\n",
        "    print('found a match')\n",
        "  else:\n",
        "    print('no match')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-PM_CxcY1H7",
        "outputId": "3bd235a7-6231-49da-af0b-e7a4b8b29e87"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looking for \"playing\" in \"Raju is playing outside\" -> found a match\n",
            "looking for \"dataset\" in \"Raju is playing outside\" -> no match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers"
      ],
      "metadata": {
        "id": "b8wpZYztZMea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "text = \"Let's see how it's working.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9F0op0zZN3C",
        "outputId": "e3dc4a9c-e415-47fe-ba93-ed3ddb5374c8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', 's', 'see', 'how', 'it', 's', 'working']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y53YppKkZTCO",
        "outputId": "d5d0a465-f3b5-4ffc-d951-a4ef09799f5b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', 's', 'see', 'how', 'it', 's', 'working']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7a11KLtZW6L",
        "outputId": "7f751be9-a98a-446e-e80c-58f9c5858d6d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_vocab = set(tokens)\n",
        "print(my_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUluv1REZZ42",
        "outputId": "dedd3e6d-ca36-410a-c7b8-090fa14db1ad"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'it', 'working', 'Let', 'see', 's', 'how'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(my_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHwTwPHFZd-s",
        "outputId": "a291272a-e7d3-4854-bdc1-51f2e757c292"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_st = \"Ther is no need to panic. We need to work together, take small yet important measures.\"\n"
      ],
      "metadata": {
        "id": "qpZYWeNOZk5Y"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.regexp import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "HLtU-DdUZtTn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_t = WordPunctTokenizer().tokenize(my_st)\n",
        "m_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl6JudXtZ1wf",
        "outputId": "304cb4f1-2087-4ffe-fa75-fe74adc5f264"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ther',\n",
              " 'is',\n",
              " 'no',\n",
              " 'need',\n",
              " 'to',\n",
              " 'panic',\n",
              " '.',\n",
              " 'We',\n",
              " 'need',\n",
              " 'to',\n",
              " 'work',\n",
              " 'together',\n",
              " ',',\n",
              " 'take',\n",
              " 'small',\n",
              " 'yet',\n",
              " 'important',\n",
              " 'measures',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_vocab = set(m_t)\n",
        "print(len(my_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xfck6xJZ6Y-",
        "outputId": "90acc7fd-d2c6-4e07-f948-58c20e6c060c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eYE_3f5aBLF",
        "outputId": "a111b80c-fc63-420b-cfe7-55f6689da91a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',',\n",
              " '.',\n",
              " 'Ther',\n",
              " 'We',\n",
              " 'important',\n",
              " 'is',\n",
              " 'measures',\n",
              " 'need',\n",
              " 'no',\n",
              " 'panic',\n",
              " 'small',\n",
              " 'take',\n",
              " 'to',\n",
              " 'together',\n",
              " 'work',\n",
              " 'yet'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcYEQ3LiaCW1",
        "outputId": "70f8b8d3-f080-43dd-de4d-9d08b08516c8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution"
      ],
      "metadata": {
        "id": "vRy-6g6TaJtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(\"\\n\\n\\n\")\n",
        "text1 = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
        "fd = nltk.FreqDist(word_tokenize(text1))\n",
        "print(fd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ayr2V2kaH5s",
        "outputId": "4a86c345-3d18-462b-efa6-0cbf58bd6307"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "<FreqDist with 11 samples and 12 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fd['everyone'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4igeBhjGaXwG",
        "outputId": "4f454e7b-b38e-40d4-a17d-8efb449f831b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = fd.keys()\n",
        "print(type(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXL6XIORadry",
        "outputId": "480249df-7d79-4190-f3d0-23cb05e10784"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict_keys'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpaOj0Oak-X",
        "outputId": "297f7f19-5858-493e-a41d-446dbb2bf1a0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uxAtdrs_am3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}